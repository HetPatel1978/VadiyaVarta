{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.7 MB 3.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 3.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.4/9.7 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.1/9.7 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.7 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.7/9.7 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.5/9.7 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.3/9.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.12.14 charset-normalizer-3.4.1 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.1 idna-3.10 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.0 typing-extensions-4.12.2 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (2.36.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (2.2.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (10.4.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (0.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (6.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: moviepy\n",
      "Version: 2.1.2\n",
      "Summary: Video editing with Python\n",
      "Home-page: \n",
      "Author: Zulko 2024\n",
      "Author-email: \n",
      "License: MIT License\n",
      "Location: C:\\Users\\hetpa\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: decorator, imageio, imageio_ffmpeg, numpy, pillow, proglog, python-dotenv\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (2.1.2)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (5.1.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (2.36.1)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.25.0 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (2.2.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (1.0.1)\n",
      "Requirement already satisfied: pillow<11.0,>=9.2.0 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (10.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from proglog<=1.0.0->moviepy) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->proglog<=1.0.0->moviepy) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: moviepy\n",
      "Version: 2.1.2\n",
      "Summary: Video editing with Python\n",
      "Home-page: \n",
      "Author: Zulko 2024\n",
      "Author-email: \n",
      "License: MIT License\n",
      "Location: C:\\Users\\hetpa\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: decorator, imageio, imageio_ffmpeg, numpy, pillow, proglog, python-dotenv\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Program Files\\\\Python313\\\\python313.zip', 'c:\\\\Program Files\\\\Python313\\\\DLLs', 'c:\\\\Program Files\\\\Python313\\\\Lib', 'c:\\\\Program Files\\\\Python313', '', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\win32', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\Pythonwin', 'c:\\\\Program Files\\\\Python313\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 326D-3A92\n",
      "\n",
      " Directory of c:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\n",
      "\n",
      "\n",
      " Directory of c:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\n",
      "\n",
      "\n",
      " Directory of c:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\n",
      "\n",
      "\n",
      " Directory of c:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "ls $(python -m site --user-site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "asttokens          3.0.0\n",
      "certifi            2024.12.14\n",
      "charset-normalizer 3.4.1\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "debugpy            1.8.11\n",
      "decorator          5.1.1\n",
      "executing          2.1.0\n",
      "filelock           3.16.1\n",
      "fsspec             2024.12.0\n",
      "huggingface-hub    0.27.1\n",
      "idna               3.10\n",
      "imageio            2.36.1\n",
      "imageio-ffmpeg     0.6.0\n",
      "ipykernel          6.29.5\n",
      "ipython            8.31.0\n",
      "jedi               0.19.2\n",
      "jupyter_client     8.6.3\n",
      "jupyter_core       5.7.2\n",
      "matplotlib-inline  0.1.7\n",
      "moviepy            2.1.2\n",
      "nest-asyncio       1.6.0\n",
      "numpy              2.2.1\n",
      "packaging          24.2\n",
      "pandas             2.2.3\n",
      "parso              0.8.4\n",
      "pillow             10.4.0\n",
      "pip                24.3.1\n",
      "platformdirs       4.3.6\n",
      "proglog            0.1.10\n",
      "prompt_toolkit     3.0.48\n",
      "psutil             6.1.1\n",
      "pure_eval          0.2.3\n",
      "Pygments           2.18.0\n",
      "python-dateutil    2.9.0.post0\n",
      "python-dotenv      1.0.1\n",
      "pytz               2024.2\n",
      "pywin32            308\n",
      "PyYAML             6.0.2\n",
      "pyzmq              26.2.0\n",
      "regex              2024.11.6\n",
      "requests           2.32.3\n",
      "safetensors        0.5.2\n",
      "six                1.17.0\n",
      "stack-data         0.6.3\n",
      "tokenizers         0.21.0\n",
      "tornado            6.4.2\n",
      "tqdm               4.67.1\n",
      "traitlets          5.14.3\n",
      "transformers       4.48.0\n",
      "typing_extensions  4.12.2\n",
      "tzdata             2024.2\n",
      "urllib3            2.3.0\n",
      "wcwidth            0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: moviepy\n",
      "Version: 2.1.2\n",
      "Summary: Video editing with Python\n",
      "Home-page: \n",
      "Author: Zulko 2024\n",
      "Author-email: \n",
      "License: MIT License\n",
      "Location: C:\\Users\\hetpa\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: decorator, imageio, imageio_ffmpeg, numpy, pillow, proglog, python-dotenv\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Program Files\\\\Python313\\\\python313.zip', 'c:\\\\Program Files\\\\Python313\\\\DLLs', 'c:\\\\Program Files\\\\Python313\\\\Lib', 'c:\\\\Program Files\\\\Python313', '', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\win32', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\Pythonwin', 'c:\\\\Program Files\\\\Python313\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (2.36.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (2.2.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (10.4.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (0.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (6.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: moviepy\n",
      "Version: 2.1.2\n",
      "Summary: Video editing with Python\n",
      "Home-page: \n",
      "Author: Zulko 2024\n",
      "Author-email: \n",
      "License: MIT License\n",
      "Location: C:\\Users\\hetpa\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: decorator, imageio, imageio_ffmpeg, numpy, pillow, proglog, python-dotenv\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall moviepy imageio imageio_ffmpeg numpy pillow proglog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (2.36.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (2.2.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (10.4.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (0.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from imageio[ffmpeg]) (6.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extracted to: output_audio.wav\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    try:\n",
    "        (\n",
    "            ffmpeg\n",
    "            .input(video_path)\n",
    "            .output(output_audio_path, format=\"wav\", acodec=\"pcm_s16le\", ac=1, ar=\"16000\")\n",
    "            .run(overwrite_output=True)\n",
    "        )\n",
    "        print(f\"Audio extracted to: {output_audio_path}\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Error:\", e.stderr.decode())\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Video file not found or FFmpeg is not properly installed.\")\n",
    "\n",
    "# Example usage\n",
    "extract_audio(r\"input_video.mp4\", r\"output_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.14.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from SpeechRecognition) (4.12.2)\n",
      "Collecting standard-aifc (from SpeechRecognition)\n",
      "  Downloading standard_aifc-3.13.0-py3-none-any.whl.metadata (969 bytes)\n",
      "Collecting audioop-lts (from SpeechRecognition)\n",
      "  Downloading audioop_lts-0.2.1-cp313-abi3-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting standard-chunk (from standard-aifc->SpeechRecognition)\n",
      "  Downloading standard_chunk-3.13.0-py3-none-any.whl.metadata (860 bytes)\n",
      "Downloading SpeechRecognition-3.14.0-py3-none-any.whl (32.9 MB)\n",
      "   ---------------------------------------- 0.0/32.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/32.9 MB 5.5 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.6/32.9 MB 5.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.9/32.9 MB 5.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.9/32.9 MB 5.3 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 5.2/32.9 MB 5.3 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 6.3/32.9 MB 5.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 7.3/32.9 MB 5.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 8.7/32.9 MB 5.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 9.7/32.9 MB 5.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 11.0/32.9 MB 5.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 12.1/32.9 MB 5.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 13.4/32.9 MB 5.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 14.4/32.9 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 15.5/32.9 MB 5.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 16.8/32.9 MB 5.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 17.8/32.9 MB 5.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 18.9/32.9 MB 5.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 19.9/32.9 MB 5.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 20.7/32.9 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 21.5/32.9 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 22.5/32.9 MB 5.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.3/32.9 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 24.4/32.9 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 24.9/32.9 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 25.7/32.9 MB 4.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.5/32.9 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 27.0/32.9 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.8/32.9 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.6/32.9 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 29.4/32.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 30.1/32.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.9/32.9 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.7/32.9 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.5/32.9 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 32.9/32.9 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading audioop_lts-0.2.1-cp313-abi3-win_amd64.whl (30 kB)\n",
      "Downloading standard_aifc-3.13.0-py3-none-any.whl (10 kB)\n",
      "Downloading standard_chunk-3.13.0-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: standard-chunk, pydub, audioop-lts, standard-aifc, SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.14.0 audioop-lts-0.2.1 pydub-0.25.1 standard-aifc-3.13.0 standard-chunk-3.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extracted to: output_audio.wav\n",
      "Loading audio file...\n",
      "Converting audio to text...\n",
      "Text extracted successfully!\n",
      "Extracted Text: hello doctor for the past few days I have been feeling very unwell I have been experiencing constant headaches and a feeling of extreme weakness I also have mild fever that comes and goes sometimes I feel dizzy specially when I stand up quickly along with that I have a loss of appetite and my throat feels a bit sir I am not sure what's causing this symptoms and I would like your advice on what to do next\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Function to extract audio from video\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    try:\n",
    "        (\n",
    "            ffmpeg\n",
    "            .input(video_path)\n",
    "            .output(output_audio_path, format=\"wav\", acodec=\"pcm_s16le\", ac=1, ar=\"16000\")\n",
    "            .run(overwrite_output=True)\n",
    "        )\n",
    "        print(f\"Audio extracted to: {output_audio_path}\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Error:\", e.stderr.decode())\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Video file not found or FFmpeg is not properly installed.\")\n",
    "\n",
    "# Function to extract text from audio\n",
    "def audio_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()  # Initialize the speech recognizer\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        with sr.AudioFile(audio_path) as source:\n",
    "            print(\"Loading audio file...\")\n",
    "            audio = recognizer.record(source)  # Read the entire audio file\n",
    "\n",
    "        # Perform speech recognition\n",
    "        print(\"Converting audio to text...\")\n",
    "        text = recognizer.recognize_google(audio)  # Use Google's speech-to-text\n",
    "        print(\"Text extracted successfully!\")\n",
    "        return text\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand the audio.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results from Google Speech Recognition service; {e}\"\n",
    "\n",
    "# Main Execution\n",
    "video_file = \"input_video.mp4\"      # Path to your video file\n",
    "audio_file = \"output_audio.wav\"    # Path to save the extracted audio file\n",
    "\n",
    "# Step 1: Extract audio from video\n",
    "extract_audio(video_file, audio_file)\n",
    "\n",
    "# Step 2: Extract text from the audio\n",
    "text = audio_to_text(audio_file)\n",
    "print(\"Extracted Text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (0.2.0)\n",
      "Requirement already satisfied: speechrecognition in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (3.14.0)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: future in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from speechrecognition) (4.12.2)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from speechrecognition) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from speechrecognition) (0.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from opencv-python) (2.2.1)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from standard-aifc->speechrecognition) (3.13.0)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "   ---------------------------------------- 0.0/39.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/39.5 MB 4.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 2.1/39.5 MB 5.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.1/39.5 MB 5.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 4.5/39.5 MB 5.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 5.5/39.5 MB 5.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.8/39.5 MB 5.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 7.9/39.5 MB 5.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.2/39.5 MB 5.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.2/39.5 MB 5.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.5/39.5 MB 5.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 12.8/39.5 MB 5.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 13.9/39.5 MB 5.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 15.2/39.5 MB 5.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.3/39.5 MB 5.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 17.6/39.5 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 18.6/39.5 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.7/39.5 MB 5.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.0/39.5 MB 5.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.0/39.5 MB 5.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 23.1/39.5 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 24.4/39.5 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 25.4/39.5 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 26.7/39.5 MB 5.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.8/39.5 MB 5.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 29.1/39.5 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.1/39.5 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 31.5/39.5 MB 5.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.8/39.5 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.8/39.5 MB 5.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 35.1/39.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 36.2/39.5 MB 5.6 MB/s eta 0:00:01\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ~~~~~~~~~~~~~^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ~~~~~~~~~~~~~~^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\http\\client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        partially_downloaded_reqs,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        parallel_builds=parallel_builds,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ~~~~~~~~~~~~~~~~~~~^\n",
      "        chunk_size,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<22 lines>...\n",
      "        decode_content=False,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\", ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python speechrecognition opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Recorded Video...\n",
      "Audio extracted to: output_audio.wav\n",
      "Loading audio file...\n",
      "Converting audio to text...\n",
      "Text extracted successfully!\n",
      "Extracted Text from Recorded Video: hello doctor for the past few days I have been feeling very unwell I have been experiencing constant headaches and a feeling of extreme weakness I also have mild fever that comes and goes sometimes I feel dizzy specially when I stand up quickly along with that I have a loss of appetite and my throat feels a bit sir I am not sure what's causing this symptoms and I would like your advice on what to do next\n",
      "Processing Live Video...\n",
      "Starting live video...\n",
      "Error accessing webcam!\n",
      "Recording live audio...\n",
      "Recording complete.\n",
      "Transcribing Recorded Live Audio...\n",
      "Loading audio file...\n",
      "Converting audio to text...\n",
      "Text extracted successfully!\n",
      "Extracted Text from Live Video Audio: aur Agar live Wala Aaya\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import speech_recognition as sr\n",
    "import cv2\n",
    "import wave\n",
    "import pyaudio\n",
    "import threading\n",
    "\n",
    "# Function to extract audio from a recorded video\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    try:\n",
    "        (\n",
    "            ffmpeg\n",
    "            .input(video_path)\n",
    "            .output(output_audio_path, format=\"wav\", acodec=\"pcm_s16le\", ac=1, ar=\"16000\")\n",
    "            .run(overwrite_output=True)\n",
    "        )\n",
    "        print(f\"Audio extracted to: {output_audio_path}\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Error:\", e.stderr.decode())\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Video file not found or FFmpeg is not properly installed.\")\n",
    "\n",
    "# Function to transcribe audio to text\n",
    "def audio_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        with sr.AudioFile(audio_path) as source:\n",
    "            print(\"Loading audio file...\")\n",
    "            audio = recognizer.record(source)\n",
    "        print(\"Converting audio to text...\")\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"Text extracted successfully!\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand the audio.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results from Google Speech Recognition service; {e}\"\n",
    "\n",
    "# Function to handle live video feed\n",
    "def live_video_with_audio_recording(output_audio_path, record_time=10):\n",
    "    def record_audio():\n",
    "        \"\"\"Record audio from the microphone.\"\"\"\n",
    "        chunk = 1024\n",
    "        sample_format = pyaudio.paInt16\n",
    "        channels = 1\n",
    "        rate = 16000\n",
    "\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=sample_format, channels=channels, rate=rate, input=True, frames_per_buffer=chunk)\n",
    "\n",
    "        frames = []\n",
    "        print(\"Recording live audio...\")\n",
    "        for _ in range(0, int(rate / chunk * record_time)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"Recording complete.\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        # Save recorded audio to a file\n",
    "        with wave.open(output_audio_path, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "\n",
    "    # Start video feed\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    print(\"Starting live video...\")\n",
    "\n",
    "    # Record audio in a separate thread\n",
    "    audio_thread = threading.Thread(target=record_audio)\n",
    "    audio_thread.start()\n",
    "\n",
    "    while audio_thread.is_alive():  # Keep showing video feed while recording audio\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            print(\"Error accessing webcam!\")\n",
    "            break\n",
    "        cv2.imshow(\"Live Video Feed\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    audio_thread.join()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main Execution\n",
    "video_file = \"input_video.mp4\"\n",
    "audio_file_recorded = \"output_audio.wav\"\n",
    "audio_file_live = \"live_audio.wav\"\n",
    "\n",
    "# Step 1: Process recorded video\n",
    "print(\"Processing Recorded Video...\")\n",
    "extract_audio(video_file, audio_file_recorded)\n",
    "text_recorded = audio_to_text(audio_file_recorded)\n",
    "print(\"Extracted Text from Recorded Video:\", text_recorded)\n",
    "\n",
    "# Step 2: Process live video with audio recording\n",
    "print(\"Processing Live Video...\")\n",
    "live_video_with_audio_recording(audio_file_live, record_time=10)\n",
    "\n",
    "# Step 3: Transcribe the recorded live audio\n",
    "print(\"Transcribing Recorded Live Audio...\")\n",
    "text_live = audio_to_text(audio_file_live)\n",
    "print(\"Extracted Text from Live Video Audio:\", text_live)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp313-cp313-win_amd64.whl.metadata (2.7 kB)\n",
      "Downloading PyAudio-0.2.14-cp313-cp313-win_amd64.whl (173 kB)\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Video-to-Text Symptom Classification Tool!\n",
      "Processing audio...\n",
      "Listening for audio...\n",
      "Starting video feed...\n",
      "\n",
      "Extracted Text: duration 10\n",
      "Symptoms Detected: \n",
      "Video feed stopped.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import ffmpeg\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "\n",
    "# Function to process audio in real-time and transcribe text\n",
    "def live_video_to_text(video_source, process_duration, symptoms_to_check):\n",
    "    \"\"\"\n",
    "    Directly converts live video or recorded video audio to text.\n",
    "    :param video_source: 0 for live webcam, or file path for a video file.\n",
    "    :param process_duration: Duration for audio processing in seconds.\n",
    "    :param symptoms_to_check: List of symptoms to classify based on text.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    stop_flag = threading.Event()\n",
    "\n",
    "    def audio_processing():\n",
    "        print(\"Processing audio...\")\n",
    "        with mic as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)  # Reduce background noise\n",
    "            try:\n",
    "                print(\"Listening for audio...\")\n",
    "                audio = recognizer.listen(source, timeout=process_duration)\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                print(\"\\nExtracted Text:\", text)\n",
    "\n",
    "                # Perform symptom classification\n",
    "                detected_symptoms = [symptom for symptom in symptoms_to_check if symptom.lower() in text.lower()]\n",
    "                if detected_symptoms:\n",
    "                    print(\"Symptoms Detected:\", \", \".join(detected_symptoms))\n",
    "                else:\n",
    "                    print(\"No specified symptoms detected.\")\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand the audio.\")\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"Google API Error: {e}\")\n",
    "\n",
    "        stop_flag.set()  # Stop the video feed after processing audio\n",
    "\n",
    "    # Thread for audio processing\n",
    "    audio_thread = threading.Thread(target=audio_processing)\n",
    "    audio_thread.start()\n",
    "\n",
    "    # Open video stream\n",
    "    capture = cv2.VideoCapture(video_source)\n",
    "    if not capture.isOpened():\n",
    "        print(\"Error: Cannot open video source.\")\n",
    "        return\n",
    "\n",
    "    print(\"Starting video feed...\")\n",
    "    while not stop_flag.is_set():  # Run the video feed while audio is being processed\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            print(\"Video source ended or cannot be read.\")\n",
    "            break\n",
    "        cv2.imshow(\"Video Feed\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_flag.set()\n",
    "            break\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Video feed stopped.\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Video-to-Text Symptom Classification Tool!\")\n",
    "\n",
    "    # Get video source input\n",
    "    video_source = input(\"Enter '0' for live webcam or provide the path to a video file: \")\n",
    "    if video_source.isdigit():\n",
    "        video_source = int(video_source)  # Convert to int for webcam\n",
    "\n",
    "    # Get process duration\n",
    "    try:\n",
    "        process_duration = int(input(\"Enter the duration (in seconds) for audio processing: \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Defaulting to 10 seconds.\")\n",
    "        process_duration = 10\n",
    "\n",
    "    # Get symptoms to check\n",
    "    symptoms_input = input(\"Enter symptoms to classify (comma-separated): \")\n",
    "    symptoms_to_check = [symptom.strip() for symptom in symptoms_input.split(\",\")]\n",
    "\n",
    "    # Run the video-to-text conversion\n",
    "    live_video_to_text(video_source, process_duration, symptoms_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Words: hi I want to talk to doctor\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import wave\n",
    "import pyaudio\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Function to handle live video feed with audio recording\n",
    "def live_video_with_audio_recording(output_audio_path, record_time=10):\n",
    "    def record_audio():\n",
    "        \"\"\"Record audio from the microphone.\"\"\"\n",
    "        chunk = 1024\n",
    "        sample_format = pyaudio.paInt16\n",
    "        channels = 1\n",
    "        rate = 16000\n",
    "\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=sample_format, channels=channels, rate=rate, input=True, frames_per_buffer=chunk)\n",
    "\n",
    "        frames = []\n",
    "        for _ in range(0, int(rate / chunk * record_time)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        # Save recorded audio to a file\n",
    "        with wave.open(output_audio_path, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "\n",
    "    # Start video feed\n",
    "    capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # Record audio in a separate thread\n",
    "    audio_thread = threading.Thread(target=record_audio)\n",
    "    audio_thread.start()\n",
    "\n",
    "    while audio_thread.is_alive():  # Keep showing video feed while recording audio\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imshow(\"Live Video Feed\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    audio_thread.join()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to transcribe audio to text\n",
    "def audio_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        with sr.AudioFile(audio_path) as source:\n",
    "            audio = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"Extracted Words:\", text)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand the audio.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results from Google Speech Recognition service; {e}\"\n",
    "\n",
    "# Main Execution for Live Video\n",
    "if __name__ == \"__main__\":\n",
    "    time.sleep(5)  # Countdown before starting\n",
    "    audio_file_live = \"live_audio.wav\"\n",
    "    live_video_with_audio_recording(audio_file_live, record_time=10)\n",
    "    audio_to_text(audio_file_live)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (4.48.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torchaudio (from versions: none)\n",
      "ERROR: No matching distribution found for torchaudio\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torchaudio librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (2.2.1)\n",
      "Collecting scipy>=1.2.0 (from librosa)\n",
      "  Downloading scipy-1.15.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn>=0.20.0 (from librosa)\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting joblib>=0.14 (from librosa)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.60.0.tar.gz (2.7 MB)\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "     --------------- ------------------------ 1.0/2.7 MB 2.6 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.6/2.7 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.1/2.7 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.7/2.7 MB 2.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "   Getting requirements to build wheel did not run successfully.\n",
      "   exit code: 1\n",
      "  > [24 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "          ~~~~^^\n",
      "        File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Program Files\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "        File \"C:\\Users\\hetpa\\AppData\\Local\\Temp\\pip-build-env-j1hxr20y\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\hetpa\\AppData\\Local\\Temp\\pip-build-env-j1hxr20y\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n",
      "          self.run_setup()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "        File \"C:\\Users\\hetpa\\AppData\\Local\\Temp\\pip-build-env-j1hxr20y\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "          ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\hetpa\\AppData\\Local\\Temp\\pip-build-env-j1hxr20y\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "          ~~~~^^^^^^^^^^^^^^^^\n",
      "        File \"<string>\", line 51, in <module>\n",
      "        File \"<string>\", line 48, in _guard_py_ver\n",
      "      RuntimeError: Cannot install on Python version 3.13.1; only versions >=3.9,<3.13 are supported.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      " Getting requirements to build wheel did not run successfully.\n",
      " exit code: 1\n",
      "> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hetpa\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (5135, 25)\n",
      "Columns in combined dataset: Index(['Disease', 'Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4',\n",
      "       'Symptom_5', 'Symptom_6', 'Symptom_7', 'Symptom_8', 'Symptom_9',\n",
      "       'Symptom_10', 'Symptom_11', 'Symptom_12', 'Symptom_13', 'Symptom_14',\n",
      "       'Symptom_15', 'Symptom_16', 'Symptom_17', 'Symptom', 'weight',\n",
      "       'Description', 'Precaution_1', 'Precaution_2', 'Precaution_3',\n",
      "       'Precaution_4'],\n",
      "      dtype='object')\n",
      "\n",
      "Checking for missing values:\n",
      "Disease          133\n",
      "Symptom_1        215\n",
      "Symptom_2        215\n",
      "Symptom_3        215\n",
      "Symptom_4        563\n",
      "Symptom_5       1421\n",
      "Symptom_6       2201\n",
      "Symptom_7       2867\n",
      "Symptom_8       3191\n",
      "Symptom_9       3443\n",
      "Symptom_10      3623\n",
      "Symptom_11      3941\n",
      "Symptom_12      4391\n",
      "Symptom_13      4631\n",
      "Symptom_14      4829\n",
      "Symptom_15      4895\n",
      "Symptom_16      4943\n",
      "Symptom_17      5063\n",
      "Symptom         5002\n",
      "weight          5002\n",
      "Description     5094\n",
      "Precaution_1    5094\n",
      "Precaution_2    5094\n",
      "Precaution_3    5095\n",
      "Precaution_4    5095\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot set a DataFrame with multiple columns to the single column All_Symptoms",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17468\\321067215.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Convert all symptom columns to strings and fill missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymptom_cols\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymptom_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Combine symptoms into a single column as a string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'All_Symptoms'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymptom_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Remove duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4297\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4298\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4300\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4301\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4302\u001b[0m         elif (\n\u001b[0;32m   4303\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4304\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4456\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4458\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4459\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   4460\u001b[0m                 \u001b[1;34m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4461\u001b[0m                 \u001b[1;33mf\"\u001b[0m\u001b[1;33mcolumn \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4462\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set a DataFrame with multiple columns to the single column All_Symptoms"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Load all files from the folder\n",
    "folder_path = r\"C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\"\n",
    "all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Combine all files into a single DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in all_files]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Step 2: Check the combined dataset\n",
    "print(\"Combined dataset shape:\", df.shape)\n",
    "print(\"Columns in combined dataset:\", df.columns)\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows with too many missing values\n",
    "df = df.dropna(thresh=len(df.columns) - 2)  # Allow up to 2 missing columns\n",
    "\n",
    "# Identify symptom columns (ensure they start with \"Symptom\")\n",
    "symptom_cols = [col for col in df.columns if \"Symptom\" in col]\n",
    "\n",
    "# Convert all symptom columns to strings and fill missing values\n",
    "df[symptom_cols] = df[symptom_cols].fillna(\"unknown\").astype(str)\n",
    "\n",
    "# Combine symptoms into a single column as a string\n",
    "df['All_Symptoms'] = df[symptom_cols].apply(lambda row: ', '.join(row.values), axis=1)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Standardize text columns\n",
    "def standardize_text(column):\n",
    "    return column.str.lower().str.strip().str.replace(\"_\", \" \").str.replace(\"-\", \" \")\n",
    "\n",
    "text_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in text_columns:\n",
    "    df[col] = standardize_text(df[col])\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_file_path = \"path_to_your_folder/cleaned_combined_dataset.csv\"\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"\\nCleaned and combined dataset saved to: {output_file_path}\")\n",
    "print(\"Final dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptom columns: ['Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4', 'Symptom_5', 'Symptom_6', 'Symptom_7', 'Symptom_8', 'Symptom_9', 'Symptom_10', 'Symptom_11', 'Symptom_12', 'Symptom_13', 'Symptom_14', 'Symptom_15', 'Symptom_16', 'Symptom_17', 'Symptom']\n"
     ]
    }
   ],
   "source": [
    "print(\"Symptom columns:\", symptom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Symptom_1, Symptom_2, Symptom_3, Symptom_4, Symptom_5, Symptom_6, Symptom_7, Symptom_8, Symptom_9, Symptom_10, Symptom_11, Symptom_12, Symptom_13, Symptom_14, Symptom_15, Symptom_16, Symptom_17, Symptom]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df[symptom_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Disease, Symptom_1, Symptom_2, Symptom_3, Symptom_4, Symptom_5, Symptom_6, Symptom_7, Symptom_8, Symptom_9, Symptom_10, Symptom_11, Symptom_12, Symptom_13, Symptom_14, Symptom_15, Symptom_16, Symptom_17, Symptom, weight, Description, Precaution_1, Precaution_2, Precaution_3, Precaution_4]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "Columns in the dataset: Index(['Disease', 'Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4',\n",
      "       'Symptom_5', 'Symptom_6', 'Symptom_7', 'Symptom_8', 'Symptom_9',\n",
      "       'Symptom_10', 'Symptom_11', 'Symptom_12', 'Symptom_13', 'Symptom_14',\n",
      "       'Symptom_15', 'Symptom_16', 'Symptom_17', 'Symptom', 'weight',\n",
      "       'Description', 'Precaution_1', 'Precaution_2', 'Precaution_3',\n",
      "       'Precaution_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(\"Columns in the dataset:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\dataset.csv, Shape: (4920, 18)\n",
      "            Disease   Symptom_1              Symptom_2              Symptom_3  \\\n",
      "0  Fungal infection     itching              skin_rash   nodal_skin_eruptions   \n",
      "1  Fungal infection   skin_rash   nodal_skin_eruptions    dischromic _patches   \n",
      "2  Fungal infection     itching   nodal_skin_eruptions    dischromic _patches   \n",
      "3  Fungal infection     itching              skin_rash    dischromic _patches   \n",
      "4  Fungal infection     itching              skin_rash   nodal_skin_eruptions   \n",
      "\n",
      "              Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8 Symptom_9  \\\n",
      "0   dischromic _patches       NaN       NaN       NaN       NaN       NaN   \n",
      "1                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "2                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "3                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "4                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "  Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "  Symptom_16 Symptom_17  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "File: C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\Symptom-severity.csv, Shape: (133, 2)\n",
      "                Symptom  weight\n",
      "0               itching       1\n",
      "1             skin_rash       3\n",
      "2  nodal_skin_eruptions       4\n",
      "3   continuous_sneezing       4\n",
      "4             shivering       5\n",
      "File: C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\symptom_Description.csv, Shape: (41, 2)\n",
      "          Disease                                        Description\n",
      "0   Drug Reaction  An adverse drug reaction (ADR) is an injury ca...\n",
      "1         Malaria  An infectious disease caused by protozoan para...\n",
      "2         Allergy  An allergy is an immune system response to a f...\n",
      "3  Hypothyroidism  Hypothyroidism, also called underactive thyroi...\n",
      "4       Psoriasis  Psoriasis is a common skin disorder that forms...\n",
      "File: C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\symptom_precaution.csv, Shape: (41, 5)\n",
      "          Disease                      Precaution_1  \\\n",
      "0   Drug Reaction                   stop irritation   \n",
      "1         Malaria          Consult nearest hospital   \n",
      "2         Allergy                    apply calamine   \n",
      "3  Hypothyroidism                     reduce stress   \n",
      "4       Psoriasis  wash hands with warm soapy water   \n",
      "\n",
      "                   Precaution_2        Precaution_3  \\\n",
      "0      consult nearest hospital    stop taking drug   \n",
      "1               avoid oily food  avoid non veg food   \n",
      "2       cover area with bandage                 NaN   \n",
      "3                      exercise         eat healthy   \n",
      "4  stop bleeding using pressure      consult doctor   \n",
      "\n",
      "                  Precaution_4  \n",
      "0                    follow up  \n",
      "1           keep mosquitos out  \n",
      "2  use ice to compress itching  \n",
      "3             get proper sleep  \n",
      "4                   salt baths  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder path and files\n",
    "folder_path = r\"C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\"\n",
    "all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Check each file\n",
    "for file in all_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    print(f\"File: {file}, Shape: {temp_df.shape}\")\n",
    "    print(temp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\cleaned_dataset.csv\n",
      "Final dataset shape: (304, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Handle Missing Values\n",
    "# Fill NaN in symptom columns with 'unknown'\n",
    "symptom_cols = [col for col in df.columns if \"Symptom\" in col]\n",
    "df[symptom_cols] = df[symptom_cols].fillna(\"unknown\")\n",
    "\n",
    "# Step 2: Combine Symptoms into a Single Column\n",
    "df[\"All_Symptoms\"] = df[symptom_cols].apply(lambda row: ', '.join(row.values), axis=1)\n",
    "\n",
    "# Step 3: Standardize Text\n",
    "def standardize_text(text):\n",
    "    return text.lower().strip().replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "\n",
    "# Standardize disease and symptoms\n",
    "df[\"Disease\"] = df[\"Disease\"].apply(standardize_text)\n",
    "df[\"All_Symptoms\"] = df[\"All_Symptoms\"].apply(standardize_text)\n",
    "\n",
    "# Step 4: Remove Duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Step 5: Save the Cleaned Dataset\n",
    "output_path = r\"C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\cleaned_dataset.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "print(\"Final dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Disease   Symptom_1              Symptom_2              Symptom_3  \\\n",
      "0  fungal infection     itching              skin_rash   nodal_skin_eruptions   \n",
      "1  fungal infection   skin_rash   nodal_skin_eruptions    dischromic _patches   \n",
      "2  fungal infection     itching   nodal_skin_eruptions    dischromic _patches   \n",
      "3  fungal infection     itching              skin_rash    dischromic _patches   \n",
      "4  fungal infection     itching              skin_rash   nodal_skin_eruptions   \n",
      "\n",
      "              Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8 Symptom_9  \\\n",
      "0   dischromic _patches   unknown   unknown   unknown   unknown   unknown   \n",
      "1               unknown   unknown   unknown   unknown   unknown   unknown   \n",
      "2               unknown   unknown   unknown   unknown   unknown   unknown   \n",
      "3               unknown   unknown   unknown   unknown   unknown   unknown   \n",
      "4               unknown   unknown   unknown   unknown   unknown   unknown   \n",
      "\n",
      "  Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n",
      "0    unknown    unknown    unknown    unknown    unknown    unknown   \n",
      "1    unknown    unknown    unknown    unknown    unknown    unknown   \n",
      "2    unknown    unknown    unknown    unknown    unknown    unknown   \n",
      "3    unknown    unknown    unknown    unknown    unknown    unknown   \n",
      "4    unknown    unknown    unknown    unknown    unknown    unknown   \n",
      "\n",
      "  Symptom_16 Symptom_17                                       All_Symptoms  \n",
      "0    unknown    unknown  itching,  skin rash,  nodal skin eruptions,  d...  \n",
      "1    unknown    unknown  skin rash,  nodal skin eruptions,  dischromic ...  \n",
      "2    unknown    unknown  itching,  nodal skin eruptions,  dischromic  p...  \n",
      "3    unknown    unknown  itching,  skin rash,  dischromic  patches, unk...  \n",
      "4    unknown    unknown  itching,  skin rash,  nodal skin eruptions, un...  \n"
     ]
    }
   ],
   "source": [
    "df_cleaned = pd.read_csv(output_path)\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows before removal: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicate rows before removal: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Disease, Symptom_1, Symptom_2, Symptom_3, Symptom_4, Symptom_5, Symptom_6, Symptom_7, Symptom_8, Symptom_9, Symptom_10, Symptom_11, Symptom_12, Symptom_13, Symptom_14, Symptom_15, Symptom_16, Symptom_17, All_Symptoms]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicates = df[df.duplicated()]\n",
    "print(duplicates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per row:\n",
      "0    304\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values per row:\")\n",
    "print(df.isnull().sum(axis=1).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"Disease\", \"All_Symptoms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(thresh=len(df.columns)-2)  # Only allow up to 2 NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (304, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.20.1%2Bcpu-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.5.1%2Bcpu-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hetpa\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting numpy\n",
      "  Downloading https://download.pytorch.org/whl/numpy-1.26.3-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading https://download.pytorch.org/whl/pillow-10.2.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: pillow, numpy, torchvision, torchaudio\n",
      "Successfully installed numpy-1.26.3 pillow-10.2.0 torchaudio-2.5.1+cpu torchvision-0.20.1+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(train_labels))\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Step 5: Set Training Arguments\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save the best model based on evaluation\u001b[39;49;00m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use accuracy for model selection\u001b[39;49;00m\n\u001b[0;32m     62\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     65\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     66\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: (p\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m p\u001b[38;5;241m.\u001b[39mlabel_ids)\u001b[38;5;241m.\u001b[39mmean()}\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Step 6: Train and Evaluate\u001b[39;00m\n",
      "File \u001b[1;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1772\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32mc:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2294\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2170\u001b[0m         )\n\u001b[0;32m   2171\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "data_path = r\"C:\\Users\\hetpa\\OneDrive\\Desktop\\AIML\\Last year project\\dataset\\cleaned_dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Split dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"All_Symptoms\"].tolist(),\n",
    "    df[\"Disease\"].astype(\"category\").cat.codes.tolist(),  # Encode labels\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize the text data using the tokenizer\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)\n",
    "\n",
    "# Step 3: Convert to Hugging Face Datasets format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": train_labels\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": val_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    \"labels\": val_labels\n",
    "})\n",
    "\n",
    "# Step 4: Load Pretrained Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(set(train_labels))\n",
    ")\n",
    "\n",
    "# Step 5: Set Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,  # Save the best model based on evaluation\n",
    "    metric_for_best_model=\"accuracy\",  # Use accuracy for model selection\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(axis=-1) == p.label_ids).mean()}\n",
    ")\n",
    "\n",
    "# Step 6: Train and Evaluate\n",
    "trainer.train()\n",
    "\n",
    "# Save the Model\n",
    "model.save_pretrained(\"./trained_disease_model\")\n",
    "tokenizer.save_pretrained(\"./trained_disease_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'datasets' (c:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_metric\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the accuracy metric\u001b[39;00m\n\u001b[0;32m      4\u001b[0m accuracy_metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (c:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_accuracy(p):\n",
    "    predictions, labels = p\n",
    "    preds = predictions.argmax(axis=-1)  # Get the predicted class index\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246945 entries, 0 to 246944\n",
      "Columns: 378 entries, diseases to neck weakness\n",
      "dtypes: int64(377), object(1)\n",
      "memory usage: 712.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:/Users/hetpa/OneDrive/Desktop/AIML/Last year project/dataset/dataset2.csv\")\n",
    "print(df.info())  # Shows columns, non-null count, and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['diseases', 'anxiety and nervousness', 'depression',\n",
      "       'shortness of breath', 'depressive or psychotic symptoms',\n",
      "       'sharp chest pain', 'dizziness', 'insomnia',\n",
      "       'abnormal involuntary movements', 'chest tightness',\n",
      "       ...\n",
      "       'stuttering or stammering', 'problems with orgasm', 'nose deformity',\n",
      "       'lump over jaw', 'sore in nose', 'hip weakness', 'back swelling',\n",
      "       'ankle stiffness or tightness', 'ankle weakness', 'neck weakness'],\n",
      "      dtype='object', length=378)\n",
      "378\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:/Users/hetpa/OneDrive/Desktop/AIML/Last year project/dataset/dataset2.csv\")\n",
    "print(df.columns)  # Prints column names\n",
    "print(len(df.columns))  # Prints number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (246945, 378)\n",
      "\n",
      "Columns: ['diseases', 'anxiety and nervousness', 'depression', 'shortness of breath', 'depressive or psychotic symptoms', 'sharp chest pain', 'dizziness', 'insomnia', 'abnormal involuntary movements', 'chest tightness', 'palpitations', 'irregular heartbeat', 'breathing fast', 'hoarse voice', 'sore throat', 'difficulty speaking', 'cough', 'nasal congestion', 'throat swelling', 'diminished hearing', 'lump in throat', 'throat feels tight', 'difficulty in swallowing', 'skin swelling', 'retention of urine', 'groin mass', 'leg pain', 'hip pain', 'suprapubic pain', 'blood in stool', 'lack of growth', 'emotional symptoms', 'elbow weakness', 'back weakness', 'pus in sputum', 'symptoms of the scrotum and testes', 'swelling of scrotum', 'pain in testicles', 'flatulence', 'pus draining from ear', 'jaundice', 'mass in scrotum', 'white discharge from eye', 'irritable infant', 'abusing alcohol', 'fainting', 'hostile behavior', 'drug abuse', 'sharp abdominal pain', 'feeling ill', 'vomiting', 'headache', 'nausea', 'diarrhea', 'vaginal itching', 'vaginal dryness', 'painful urination', 'involuntary urination', 'pain during intercourse', 'frequent urination', 'lower abdominal pain', 'vaginal discharge', 'blood in urine', 'hot flashes', 'intermenstrual bleeding', 'hand or finger pain', 'wrist pain', 'hand or finger swelling', 'arm pain', 'wrist swelling', 'arm stiffness or tightness', 'arm swelling', 'hand or finger stiffness or tightness', 'wrist stiffness or tightness', 'lip swelling', 'toothache', 'abnormal appearing skin', 'skin lesion', 'acne or pimples', 'dry lips', 'facial pain', 'mouth ulcer', 'skin growth', 'eye deviation', 'diminished vision', 'double vision', 'cross-eyed', 'symptoms of eye', 'pain in eye', 'eye moves abnormally', 'abnormal movement of eyelid', 'foreign body sensation in eye', 'irregular appearing scalp', 'swollen lymph nodes', 'back pain', 'neck pain', 'low back pain', 'pain of the anus', 'pain during pregnancy', 'pelvic pain', 'impotence', 'infant spitting up', 'vomiting blood', 'regurgitation', 'burning abdominal pain', 'restlessness', 'symptoms of infants', 'wheezing', 'peripheral edema', 'neck mass', 'ear pain', 'jaw swelling', 'mouth dryness', 'neck swelling', 'knee pain', 'foot or toe pain', 'bowlegged or knock-kneed', 'ankle pain', 'bones are painful', 'knee weakness', 'elbow pain', 'knee swelling', 'skin moles', 'knee lump or mass', 'weight gain', 'problems with movement', 'knee stiffness or tightness', 'leg swelling', 'foot or toe swelling', 'heartburn', 'smoking problems', 'muscle pain', 'infant feeding problem', 'recent weight loss', 'problems with shape or size of breast', 'underweight', 'difficulty eating', 'scanty menstrual flow', 'vaginal pain', 'vaginal redness', 'vulvar irritation', 'weakness', 'decreased heart rate', 'increased heart rate', 'bleeding or discharge from nipple', 'ringing in ear', 'plugged feeling in ear', 'itchy ear(s)', 'frontal headache', 'fluid in ear', 'neck stiffness or tightness', 'spots or clouds in vision', 'eye redness', 'lacrimation', 'itchiness of eye', 'blindness', 'eye burns or stings', 'itchy eyelid', 'feeling cold', 'decreased appetite', 'excessive appetite', 'excessive anger', 'loss of sensation', 'focal weakness', 'slurring words', 'symptoms of the face', 'disturbance of memory', 'paresthesia', 'side pain', 'fever', 'shoulder pain', 'shoulder stiffness or tightness', 'shoulder weakness', 'arm cramps or spasms', 'shoulder swelling', 'tongue lesions', 'leg cramps or spasms', 'abnormal appearing tongue', 'ache all over', 'lower body pain', 'problems during pregnancy', 'spotting or bleeding during pregnancy', 'cramps and spasms', 'upper abdominal pain', 'stomach bloating', 'changes in stool appearance', 'unusual color or odor to urine', 'kidney mass', 'swollen abdomen', 'symptoms of prostate', 'leg stiffness or tightness', 'difficulty breathing', 'rib pain', 'joint pain', 'muscle stiffness or tightness', 'pallor', 'hand or finger lump or mass', 'chills', 'groin pain', 'fatigue', 'abdominal distention', 'regurgitation.1', 'symptoms of the kidneys', 'melena', 'flushing', 'coughing up sputum', 'seizures', 'delusions or hallucinations', 'shoulder cramps or spasms', 'joint stiffness or tightness', 'pain or soreness of breast', 'excessive urination at night', 'bleeding from eye', 'rectal bleeding', 'constipation', 'temper problems', 'coryza', 'wrist weakness', 'eye strain', 'hemoptysis', 'lymphedema', 'skin on leg or foot looks infected', 'allergic reaction', 'congestion in chest', 'muscle swelling', 'pus in urine', 'abnormal size or shape of ear', 'low back weakness', 'sleepiness', 'apnea', 'abnormal breathing sounds', 'excessive growth', 'elbow cramps or spasms', 'feeling hot and cold', 'blood clots during menstrual periods', 'absence of menstruation', 'pulling at ears', 'gum pain', 'redness in ear', 'fluid retention', 'flu-like syndrome', 'sinus congestion', 'painful sinuses', 'fears and phobias', 'recent pregnancy', 'uterine contractions', 'burning chest pain', 'back cramps or spasms', 'stiffness all over', 'muscle cramps, contractures, or spasms', 'low back cramps or spasms', 'back mass or lump', 'nosebleed', 'long menstrual periods', 'heavy menstrual flow', 'unpredictable menstruation', 'painful menstruation', 'infertility', 'frequent menstruation', 'sweating', 'mass on eyelid', 'swollen eye', 'eyelid swelling', 'eyelid lesion or rash', 'unwanted hair', 'symptoms of bladder', 'irregular appearing nails', 'itching of skin', 'hurts to breath', 'nailbiting', 'skin dryness, peeling, scaliness, or roughness', 'skin on arm or hand looks infected', 'skin irritation', 'itchy scalp', 'hip swelling', 'incontinence of stool', 'foot or toe cramps or spasms', 'warts', 'bumps on penis', 'too little hair', 'foot or toe lump or mass', 'skin rash', 'mass or swelling around the anus', 'low back swelling', 'ankle swelling', 'hip lump or mass', 'drainage in throat', 'dry or flaky scalp', 'premenstrual tension or irritability', 'feeling hot', 'feet turned in', 'foot or toe stiffness or tightness', 'pelvic pressure', 'elbow swelling', 'elbow stiffness or tightness', 'early or late onset of menopause', 'mass on ear', 'bleeding from ear', 'hand or finger weakness', 'low self-esteem', 'throat irritation', 'itching of the anus', 'swollen or red tonsils', 'irregular belly button', 'swollen tongue', 'lip sore', 'vulvar sore', 'hip stiffness or tightness', 'mouth pain', 'arm weakness', 'leg lump or mass', 'disturbance of smell or taste', 'discharge in stools', 'penis pain', 'loss of sex drive', 'obsessions and compulsions', 'antisocial behavior', 'neck cramps or spasms', 'pupils unequal', 'poor circulation', 'thirst', 'sleepwalking', 'skin oiliness', 'sneezing', 'bladder mass', 'knee cramps or spasms', 'premature ejaculation', 'leg weakness', 'posture problems', 'bleeding in mouth', 'tongue bleeding', 'change in skin mole size or color', 'penis redness', 'penile discharge', 'shoulder lump or mass', 'polyuria', 'cloudy eye', 'hysterical behavior', 'arm lump or mass', 'nightmares', 'bleeding gums', 'pain in gums', 'bedwetting', 'diaper rash', 'lump or mass of breast', 'vaginal bleeding after menopause', 'infrequent menstruation', 'mass on vulva', 'jaw pain', 'itching of scrotum', 'postpartum problems of the breast', 'eyelid retracted', 'hesitancy', 'elbow lump or mass', 'muscle weakness', 'throat redness', 'joint swelling', 'tongue pain', 'redness in or around nose', 'wrinkles on skin', 'foot or toe weakness', 'hand or finger cramps or spasms', 'back stiffness or tightness', 'wrist lump or mass', 'skin pain', 'low back stiffness or tightness', 'low urine output', 'skin on head or neck looks infected', 'stuttering or stammering', 'problems with orgasm', 'nose deformity', 'lump over jaw', 'sore in nose', 'hip weakness', 'back swelling', 'ankle stiffness or tightness', 'ankle weakness', 'neck weakness']\n",
      "\n",
      "Column Types:\n",
      " diseases                            object\n",
      "anxiety and nervousness              int64\n",
      "depression                           int64\n",
      "shortness of breath                  int64\n",
      "depressive or psychotic symptoms     int64\n",
      "                                     ...  \n",
      "hip weakness                         int64\n",
      "back swelling                        int64\n",
      "ankle stiffness or tightness         int64\n",
      "ankle weakness                       int64\n",
      "neck weakness                        int64\n",
      "Length: 378, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset to inspect\n",
    "df = pd.read_csv(r\"C:/Users/hetpa/OneDrive/Desktop/AIML/Last year project/dataset/dataset2.csv\")\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "print(\"\\nColumn Types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Diseases: 773\n",
      "\n",
      "Disease Distribution:\n",
      "diseases\n",
      "cystitis                          1219\n",
      "vulvodynia                        1218\n",
      "nose disorder                     1218\n",
      "complex regional pain syndrome    1217\n",
      "spondylosis                       1216\n",
      "                                  ... \n",
      "typhoid fever                        1\n",
      "rocky mountain spotted fever         1\n",
      "open wound of the knee               1\n",
      "hypergammaglobulinemia               1\n",
      "open wound due to trauma             1\n",
      "Name: count, Length: 773, dtype: int64\n",
      "\n",
      "Top 10 Most Common Symptoms:\n",
      "sharp abdominal pain    32307\n",
      "vomiting                27874\n",
      "headache                24719\n",
      "cough                   24296\n",
      "sharp chest pain        24016\n",
      "nausea                  23687\n",
      "back pain               21809\n",
      "shortness of breath     21346\n",
      "fever                   20394\n",
      "dizziness               17272\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique diseases\n",
    "print(\"\\nUnique Diseases:\", df['diseases'].nunique())\n",
    "print(\"\\nDisease Distribution:\")\n",
    "print(df['diseases'].value_counts())\n",
    "\n",
    "# Symptom prevalence\n",
    "symptom_columns = [col for col in df.columns if col != 'diseases']\n",
    "symptom_prevalence = df[symptom_columns].sum()\n",
    "print(\"\\nTop 10 Most Common Symptoms:\")\n",
    "print(symptom_prevalence.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "model_path = \"./trained_disease_model\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load label encoder to map predictions back to disease names\n",
    "label_encoder = joblib.load(f\"{model_path}/label_encoder.joblib\")\n",
    "\n",
    "def predict_disease(symptoms_list):\n",
    "    \"\"\"\n",
    "    Predicts the disease based on given symptoms.\n",
    "    :param symptoms_list: List of symptom names as strings\n",
    "    :return: Predicted disease\n",
    "    \"\"\"\n",
    "    # Convert symptoms into a single string\n",
    "    symptoms_text = \" \".join(symptoms_list)\n",
    "\n",
    "    # Tokenize input\n",
    "    encoding = tokenizer(symptoms_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "        predicted_class = torch.argmax(output.logits, dim=1).item()\n",
    "    \n",
    "    # Decode label back to disease name\n",
    "    predicted_disease = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    return predicted_disease\n",
    "\n",
    "# Example Test Case\n",
    "example_symptoms = [\"fever\", \"headache\", \"cough\"]\n",
    "predicted_disease = predict_disease(example_symptoms)\n",
    "\n",
    "print(f\"Predicted Disease: {predicted_disease}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger_eng\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hetpa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hetpa\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\hetpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlpaug'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnlpaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnaw\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 1. Data Loading and Preprocessing\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnormalize_text\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlpaug'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Data Loading and Preprocessing\n",
    "# -------------------------------\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and extra whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_prepare_dataset(csv_path):\n",
    "    \"\"\"\n",
    "    Load the CSV file, combine selected symptom columns into a single 'symptoms' field,\n",
    "    and normalize the disease names.\n",
    "    \n",
    "    Assumes the following columns in your CSV:\n",
    "    Disease, Fever, Cough, Fatigue, Difficulty Breathing, Age, Gender, Blood Pressure, Cholesterol Level, Outcome Variable\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Define symptom columns (ignore Age, Gender, etc.)\n",
    "    symptom_cols = [\"Fever\", \"Cough\", \"Fatigue\", \"Difficulty Breathing\"]\n",
    "    \n",
    "    # Convert symptom columns from \"yes\"/\"1\" to 1, else 0\n",
    "    for col in symptom_cols:\n",
    "        df[col] = df[col].apply(lambda x: 1 if str(x).strip().lower() in [\"yes\", \"1\"] else 0)\n",
    "    \n",
    "    # Combine symptom columns into one text field:\n",
    "    def combine_symptoms(row):\n",
    "        symptoms = []\n",
    "        for col in symptom_cols:\n",
    "            if row[col] == 1:\n",
    "                # Convert column name to lowercase; \"Difficulty Breathing\" becomes \"difficulty breathing\"\n",
    "                symptoms.append(col.lower())\n",
    "        return \" \".join(symptoms)\n",
    "    \n",
    "    df['symptoms'] = df.apply(combine_symptoms, axis=1)\n",
    "    \n",
    "    # Normalize the disease label and store as 'disease'\n",
    "    df['disease'] = df['Disease'].astype(str).str.lower().str.strip()\n",
    "    \n",
    "    # Keep only the relevant columns: 'disease' and 'symptoms'\n",
    "    df_final = df[['disease', 'symptoms']]\n",
    "    df_final['symptoms'] = df_final['symptoms'].apply(normalize_text)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Augmentation (Synonym Replacement)\n",
    "# -------------------------------\n",
    "\n",
    "# Using nlpaug to replace words with their synonyms (from WordNet)\n",
    "aug = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=3)\n",
    "\n",
    "def augment_text(text, n_aug=2):\n",
    "    \"\"\"Generate n_aug augmented versions of the text.\"\"\"\n",
    "    augmented_texts = []\n",
    "    for _ in range(n_aug):\n",
    "        augmented_texts.append(aug.augment(text))\n",
    "    return augmented_texts\n",
    "\n",
    "def augment_dataset(df, aug_factor=2):\n",
    "    \"\"\"\n",
    "    Augment the dataset by replacing words with synonyms.\n",
    "    For each instance, generate aug_factor extra examples.\n",
    "    \"\"\"\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Original row remains\n",
    "        augmented_rows.append(row)\n",
    "        aug_texts = augment_text(row['symptoms'], n_aug=aug_factor)\n",
    "        for text in aug_texts:\n",
    "            new_row = row.copy()\n",
    "            new_row['symptoms'] = text\n",
    "            augmented_rows.append(new_row)\n",
    "    return pd.DataFrame(augmented_rows)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Prepare Hugging Face Dataset for Training\n",
    "# -------------------------------\n",
    "\n",
    "def prepare_hf_dataset(df):\n",
    "    \"\"\"Convert pandas DataFrame to Hugging Face Dataset and tokenize.\"\"\"\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['disease'])\n",
    "    dataset = DatasetDict({\n",
    "        'train': Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "        'validation': Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Model Training with Transfer Learning and CrossValidation\n",
    "# -------------------------------\n",
    "\n",
    "# Use a domain-specific model (e.g., BioBERT)\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"symptoms\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def prepare_tokenized_dataset(dataset):\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"disease\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    return tokenized_dataset\n",
    "\n",
    "def train_model_with_kfold(df, num_labels, num_folds=5):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_results = {}\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(df), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        train_df = df.iloc[train_index]\n",
    "        val_df = df.iloc[val_index]\n",
    "        \n",
    "        # Create mapping from disease names to integer labels\n",
    "        diseases = sorted(df['disease'].unique())\n",
    "        disease2id = {d: i for i, d in enumerate(diseases)}\n",
    "        train_df['disease'] = train_df['disease'].map(disease2id)\n",
    "        val_df['disease'] = val_df['disease'].map(disease2id)\n",
    "        \n",
    "        dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_df),\n",
    "            'validation': Dataset.from_pandas(val_df)\n",
    "        })\n",
    "        \n",
    "        tokenized_dataset = prepare_tokenized_dataset(dataset)\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results_fold_{fold}\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            save_total_limit=1,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"validation\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Fold {fold} evaluation: {eval_results}\")\n",
    "        fold_results[f\"fold_{fold}\"] = eval_results\n",
    "        \n",
    "    return fold_results, disease2id\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Main Execution Pipeline\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update the path to your dataset file\n",
    "    csv_path = r\"C:/Users/hetpa/OneDrive/Desktop/AIML/Last year project/dataset/dataset3.csv\"\n",
    "    df = load_and_prepare_dataset(csv_path)\n",
    "    \n",
    "    print(\"Original dataset size:\", df.shape[0])\n",
    "    df_aug = augment_dataset(df, aug_factor=2)\n",
    "    print(\"Augmented dataset size:\", df_aug.shape[0])\n",
    "    \n",
    "    hf_dataset = prepare_hf_dataset(df_aug)\n",
    "    \n",
    "    num_labels = df_aug['disease'].nunique()\n",
    "    fold_results, disease2id = train_model_with_kfold(df_aug, num_labels, num_folds=5)\n",
    "    \n",
    "    print(\"\\nCross-validation results:\", fold_results)\n",
    "    print(\"Disease to label mapping:\", disease2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executable: c:\\Users\\hetpa\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "sys.path: ['c:\\\\Users\\\\hetpa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip', 'c:\\\\Users\\\\hetpa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib', 'c:\\\\Users\\\\hetpa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs', '', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\hetpa\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\hetpa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311', 'c:\\\\Users\\\\hetpa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"sys.path:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
